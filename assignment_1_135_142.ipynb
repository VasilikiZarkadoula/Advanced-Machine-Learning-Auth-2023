{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.datasets import  fetch_openml\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad     300\n",
      "good    700\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Retrieve german credit dataset from openml\n",
    "X, y = fetch_openml(\"credit-g\", version=1, as_frame=True, parser='auto', return_X_y=True)\n",
    "\n",
    "one_hot_encoder = make_column_transformer(\n",
    "        (OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n",
    "        make_column_selector(dtype_include='category')),\n",
    "        remainder='passthrough')\n",
    "\n",
    "X = one_hot_encoder.fit_transform(X)\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(X)\n",
    "\n",
    "# Get the class distribution for the target variable y\n",
    "class_distribution = y.value_counts().sort_index()\n",
    "\n",
    "print(class_distribution)\n",
    "\n",
    "# Define cost matrix\n",
    "cost_m = [[0, 1], \n",
    "            [5, 0]]\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "names = ['random forest', 'linear SVM', 'Naive Bayes']\n",
    "classifiers = [RandomForestClassifier(n_estimators=100, random_state=0), \n",
    "                SVC(kernel='linear',  probability=True),\n",
    "                GaussianNB()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COST MINIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "\n",
      "cost minimization without probability calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.98      0.58        90\n",
      "           1       0.98      0.40      0.56       210\n",
      "\n",
      "    accuracy                           0.57       300\n",
      "   macro avg       0.69      0.69      0.57       300\n",
      "weighted avg       0.81      0.57      0.57       300\n",
      "\n",
      "[[ 88 127]\n",
      " [  2  83]]\n",
      "the cost is:\n",
      "137\n",
      "\n",
      "cost minimization with sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.98      0.58        90\n",
      "           1       0.98      0.41      0.58       210\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.70      0.70      0.58       300\n",
      "weighted avg       0.81      0.58      0.58       300\n",
      "\n",
      "[[ 88 123]\n",
      " [  2  87]]\n",
      "the cost is:\n",
      "133\n",
      "\n",
      "cost minimization with isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.96      0.59        90\n",
      "           1       0.96      0.44      0.61       210\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.69      0.70      0.60       300\n",
      "weighted avg       0.80      0.60      0.60       300\n",
      "\n",
      "[[ 86 117]\n",
      " [  4  93]]\n",
      "the cost is:\n",
      "137\n",
      "linear SVM\n",
      "\n",
      "cost minimization without probability calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.97      0.54        90\n",
      "           1       0.96      0.30      0.46       210\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.66      0.64      0.50       300\n",
      "weighted avg       0.78      0.50      0.49       300\n",
      "\n",
      "[[ 87 146]\n",
      " [  3  64]]\n",
      "the cost is:\n",
      "161\n",
      "\n",
      "cost minimization with sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.94      0.52        90\n",
      "           1       0.92      0.26      0.41       210\n",
      "\n",
      "    accuracy                           0.47       300\n",
      "   macro avg       0.64      0.60      0.46       300\n",
      "weighted avg       0.75      0.47      0.44       300\n",
      "\n",
      "[[ 85 155]\n",
      " [  5  55]]\n",
      "the cost is:\n",
      "180\n",
      "\n",
      "cost minimization with isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.93      0.54        90\n",
      "           1       0.93      0.36      0.52       210\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.65      0.65      0.53       300\n",
      "weighted avg       0.76      0.53      0.52       300\n",
      "\n",
      "[[ 84 135]\n",
      " [  6  75]]\n",
      "the cost is:\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    print(\"\\ncost minimization without probability calibration\")\n",
    "    # Encode labels to match train and test data\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    y_pred = np.argmin(np.matmul(y_pred_prob, np.array(cost_m).T), axis=1)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "    print(conf_m) \n",
    "    print(\"the cost is:\")\n",
    "    print(np.sum(conf_m * cost_m))\n",
    "\n",
    "    print(\"\\ncost minimization with sigmoid calibration\")\n",
    "    cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "    model = cc.fit(X_train, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    y_pred = np.argmin(np.matmul(y_pred_prob, np.array(cost_m).T), axis=1)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "    print(conf_m) \n",
    "    print(\"the cost is:\")\n",
    "    print(np.sum(conf_m * cost_m))\n",
    "\n",
    "    print(\"\\ncost minimization with isotonic calibration\")\n",
    "    cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "    model = cc.fit(X_train, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    y_pred = np.argmin(np.matmul(y_pred_prob, np.array(cost_m).T), axis=1)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "    print(conf_m) \n",
    "    print(\"the cost is:\")\n",
    "    print(np.sum(conf_m * cost_m))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that in random forest and Linear SVM cost minimization without calibration performs better compared to the other two methods. However, when using the Naive Bayes classifier, the isotonic calibration approach outperforms the other two methods. Isotonic calibration minimize the cost of Naive Bayes classifier, as the classifier may not generate well-calibrated probability estimates due to its assumption of feature independence being overly simplistic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "with undersampling\n",
      "Counter({1: 294, 0: 150})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.46      0.53        90\n",
      "           1       0.79      0.89      0.84       210\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.71      0.67      0.68       300\n",
      "weighted avg       0.74      0.76      0.74       300\n",
      "\n",
      "[[ 41  24]\n",
      " [ 49 186]]\n",
      "the cost is:\n",
      "269\n",
      "\n",
      "with oversampling\n",
      "Counter({1: 588, 0: 252})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 490)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.38      0.48        90\n",
      "           1       0.78      0.92      0.84       210\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.72      0.65      0.66       300\n",
      "weighted avg       0.74      0.76      0.73       300\n",
      "\n",
      "[[ 34  17]\n",
      " [ 56 193]]\n",
      "the cost is:\n",
      "297\n",
      "\n",
      "with combination\n",
      "Counter({1: 588, 0: 252})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 294)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.42      0.51        90\n",
      "           1       0.79      0.90      0.84       210\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.72      0.66      0.68       300\n",
      "weighted avg       0.75      0.76      0.74       300\n",
      "\n",
      "[[ 38  20]\n",
      " [ 52 190]]\n",
      "the cost is:\n",
      "280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'random forest'\n",
    "print(name)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print(\"with undersampling\")\n",
    "#change the examples of class 1 to see what happens\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with oversampling\")\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with combination\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear SVM\n",
      "with undersampling\n",
      "Counter({1: 294, 0: 150})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.51      0.53        90\n",
      "           1       0.80      0.81      0.80       210\n",
      "\n",
      "    accuracy                           0.72       300\n",
      "   macro avg       0.67      0.66      0.67       300\n",
      "weighted avg       0.72      0.72      0.72       300\n",
      "\n",
      "[[ 46  39]\n",
      " [ 44 171]]\n",
      "the cost is:\n",
      "259\n",
      "\n",
      "with oversampling\n",
      "Counter({1: 588, 0: 252})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 490)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.46      0.55        90\n",
      "           1       0.80      0.91      0.85       210\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.74      0.68      0.70       300\n",
      "weighted avg       0.76      0.77      0.76       300\n",
      "\n",
      "[[ 41  19]\n",
      " [ 49 191]]\n",
      "the cost is:\n",
      "264\n",
      "\n",
      "with combination\n",
      "Counter({1: 588, 0: 252})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 294)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.44      0.49        90\n",
      "           1       0.78      0.84      0.81       210\n",
      "\n",
      "    accuracy                           0.72       300\n",
      "   macro avg       0.66      0.64      0.65       300\n",
      "weighted avg       0.71      0.72      0.71       300\n",
      "\n",
      "[[ 40  33]\n",
      " [ 50 177]]\n",
      "the cost is:\n",
      "283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'linear SVM'\n",
    "print(name)\n",
    "clf = SVC(kernel='linear',  probability=True)           \n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print(\"with undersampling\")\n",
    "#change the examples of class 1 to see what happens\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with oversampling\")\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with combination\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "with undersampling\n",
      "Counter({1: 294, 0: 150})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.93      0.45        90\n",
      "           1       0.68      0.06      0.11       210\n",
      "\n",
      "    accuracy                           0.32       300\n",
      "   macro avg       0.49      0.50      0.28       300\n",
      "weighted avg       0.57      0.32      0.22       300\n",
      "\n",
      "[[ 84 197]\n",
      " [  6  13]]\n",
      "the cost is:\n",
      "227\n",
      "\n",
      "with oversampling\n",
      "Counter({1: 588, 0: 252})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.91      0.49        90\n",
      "           1       0.85      0.22      0.35       210\n",
      "\n",
      "    accuracy                           0.43       300\n",
      "   macro avg       0.59      0.57      0.42       300\n",
      "weighted avg       0.70      0.43      0.39       300\n",
      "\n",
      "[[ 82 164]\n",
      " [  8  46]]\n",
      "the cost is:\n",
      "204\n",
      "\n",
      "with combination\n",
      "Counter({1: 588, 0: 252})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.90      0.46        90\n",
      "           1       0.76      0.13      0.23       210\n",
      "\n",
      "    accuracy                           0.36       300\n",
      "   macro avg       0.53      0.52      0.34       300\n",
      "weighted avg       0.62      0.36      0.30       300\n",
      "\n",
      "[[ 81 182]\n",
      " [  9  28]]\n",
      "the cost is:\n",
      "227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 490)\n",
      "  warnings.warn(\n",
      "c:\\Python39\\lib\\site-packages\\imblearn\\utils\\_validation.py:313: UserWarning: After over-sampling, the number of samples (588) in class 1 will be larger than the number of samples in the majority class (class #1 -> 294)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "name = 'Naive Bayes'\n",
    "print(name)\n",
    "clf = GaussianNB()\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print(\"with undersampling\")\n",
    "#change the examples of class 1 to see what happens\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with oversampling\")\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1) \n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"with combination\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 150, 1: 294}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 252, 1: 588}, random_state=1)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_m = confusion_matrix(y_test, y_pred).T # transpose to align with slides\n",
    "print(conf_m)\n",
    "loss = np.sum(conf_m * cost_m)\n",
    "print(\"the cost is:\")\n",
    "print(\"%d\\n\" %loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results obtained, it seems that in random forest and Linear SVM classifiers, the undersampling method demonstrates better results regarding the cost minimization. This tecnique removes instances from the majority class. As a result, it can rebalance the class distribution and prevent the classifier from being biased towards the majority class. However, the Naive Bayes classifier shows improved performance when using the oversampling technique, which involves duplicating instances from the minority class. This approach can improve the classifier's ability to capture characteristics of the minority class, leading to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "\n",
      "with weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.32      0.43        90\n",
      "           1       0.76      0.93      0.84       210\n",
      "\n",
      "    accuracy                           0.75       300\n",
      "   macro avg       0.71      0.63      0.63       300\n",
      "weighted avg       0.73      0.75      0.72       300\n",
      "\n",
      "[[ 29  15]\n",
      " [ 61 195]]\n",
      "the cost is:\n",
      "320\n",
      "\n",
      "linear SVM\n",
      "\n",
      "with weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.96      0.51        90\n",
      "           1       0.93      0.24      0.38       210\n",
      "\n",
      "    accuracy                           0.46       300\n",
      "   macro avg       0.64      0.60      0.45       300\n",
      "weighted avg       0.75      0.46      0.42       300\n",
      "\n",
      "[[ 86 159]\n",
      " [  4  51]]\n",
      "the cost is:\n",
      "179\n",
      "\n",
      "Naive Bayes\n",
      "\n",
      "with weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.89      0.46        90\n",
      "           1       0.76      0.15      0.25       210\n",
      "\n",
      "    accuracy                           0.37       300\n",
      "   macro avg       0.54      0.52      0.36       300\n",
      "weighted avg       0.63      0.37      0.32       300\n",
      "\n",
      "[[ 80 178]\n",
      " [ 10  32]]\n",
      "the cost is:\n",
      "228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['random forest', 'linear SVM', 'Naive Bayes']\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    print(\"\\nwith weights\")\n",
    "    # now create the sample weights according to y\n",
    "    weights = np.zeros(y_train.shape[0])\n",
    "    weights[np.where(y_train == 1)] = 1;\n",
    "    weights[np.where(y_train == 0)] = 4;\n",
    "    model = clf.fit(X_train, y_train, weights)\n",
    "    pred_test = clf.predict(X_test)\n",
    "    print(classification_report(y_test, pred_test))\n",
    "    conf_m = confusion_matrix(y_test, pred_test).T # transpose to align with slides\n",
    "    print(conf_m)\n",
    "    loss = np.sum(conf_m * cost_m)\n",
    "    print(\"the cost is:\")\n",
    "    print(\"%d\\n\" %loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the linear SVM classifier with weights demonstrates better performance compared to the random forest and Naive Bayes. In the case of linear SVM, which is a margin-based algorithm, applying weights can shift the decision boundary towards the minority class. This leads to better identification of the \"bad\" customers minimizing the cost associated with misclassification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
